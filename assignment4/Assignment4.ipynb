{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/nn_assignments/blob/nn18/assignment4/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "e8znHEqEb5dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Starter code: network for Irises in Pytorch\n",
        "\n",
        "\n",
        "In the following cells a feedforward neural network has been implemented with the aid of PyTorch and its autograd mechanism. Please study the code - many network implementations follow a similar pattern.\n",
        "\n",
        "The provided network trains to nearly 100% accuracy on Iris using Batch Gradient Descent."
      ]
    },
    {
      "metadata": {
        "id": "rFm1zlehb5dl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 4\n",
        "\n",
        "**Submission deadline:** \n",
        "\n",
        "**problems 1-2 last lab session before or on Wednesday, 30.11.18**\n",
        "\n",
        "**problems 3-8 last lab session before or on Wednesday, 7.12.18**\n",
        "\n",
        "\n",
        "**Points: 11 + 5 bonus points**\n",
        "\n",
        "\n",
        "## Downloading this notebook\n",
        "\n",
        "This assignment is an Colab/Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
        "\n",
        "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!\n",
        "\n",
        "If you use Colab please make sure to have a GPU runtime (```Runtime -> Change runtime type -> Hardware Accelarater = GPU```)"
      ]
    },
    {
      "metadata": {
        "id": "gn2GEv12c1je",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Please note that this code needs only to be run in a fresh runtime.\n",
        "# However, it can be rerun afterwards too.\n",
        "!pip install -q gdown httpimport\n",
        "![ -e cifar.npz ] || gdown 'https://drive.google.com/uc?id=1oBzZdtg2zNTPGhbRy6DQ_wrf5L5OAhNR' -O cifar.npz\n",
        "![ -e mnist.npz ] || gdown 'https://drive.google.com/uc?id=1QPaC3IKB_5tX6yIZgRgkpcqFrfVqPTXU' -O mnist.npz\n",
        "\n",
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEu2rmK0b5dn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%pylab inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZNK4WNTfUmK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting to know PyTorch\n",
        "\n",
        "From now on, we will use [pytorch](https://pytorch.org/) to implement neural networks.\n",
        "\n",
        "Good introductions are the [60-minute tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and [pytorch examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
        "\n",
        "\n",
        "Pytorch has a set of modules to build neural networks. However, for simplicity, this assignment will use a custom, simple neural network code."
      ]
    },
    {
      "metadata": {
        "id": "uMppPnb5b5dw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Layer(object):\n",
        "    \n",
        "    def train_mode(self):\n",
        "        \"\"\"Put layer into training mode.\n",
        "        \n",
        "        This is useful for e.g. Dropout.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def eval_mode(self):\n",
        "        \"\"\"Put layer into evalation mode.\n",
        "        \n",
        "        This is useful for e.g. Dropout.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def to_device(self, device):\n",
        "        \"\"\"Move this layer's parameters to thie given device (cpu/cuda).\n",
        "        \"\"\"\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    @property\n",
        "    def parameters(self):\n",
        "        return []\n",
        "    \n",
        "    def __repr__(self):\n",
        "        param_s = ', '.join(['%s%s' % (n, tuple(p.size())) \n",
        "                                 for (n,p) in self.parameters])\n",
        "        if not param_s:\n",
        "            return '  %s with no params' % (self.__class__.__name__)\n",
        "        else:\n",
        "            return '  %s with params: %s' % (self.__class__.__name__, param_s)\n",
        "    \n",
        "\n",
        "class AffineLayer(Layer):\n",
        "    def __init__(self, num_in, num_out):\n",
        "        self.W = torch.empty(num_in, num_out, dtype=torch.float32, requires_grad=True)\n",
        "        self.b = torch.empty(1, num_out, dtype=torch.float32, requires_grad=True)\n",
        "    \n",
        "    @property\n",
        "    def parameters(self):\n",
        "        return [('W', self.W), ('b', self.b)]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.mm(self.W) + self.b\n",
        "    \n",
        "    def to_device(self, device):\n",
        "        with torch.no_grad():\n",
        "            self.W = self.W.to(device).requires_grad_()\n",
        "            self.b = self.b.to(device).requires_grad_()\n",
        "\n",
        "    \n",
        "class TanhLayer(Layer):\n",
        "    def forward(self, x):\n",
        "        return F.tanh(x)\n",
        "\n",
        "    \n",
        "class  ReLULayer(Layer):\n",
        "    def forward(self, x):\n",
        "        return F.relu(x)\n",
        "\n",
        "\n",
        "class SoftMaxLayer(Layer):\n",
        "    def forward(self, x):\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "class CrossEntropyLoss:\n",
        "    def forward(self, probs, targets):\n",
        "        return torch.mean(-torch.log(torch.gather(\n",
        "            probs, 1, targets.unsqueeze(1))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6q1CdUkUb5d1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeedforwardNet(object):\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "        self.loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    @property\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params += layer.parameters\n",
        "        return params\n",
        "\n",
        "    @parameters.setter\n",
        "    def parameters(self, values):\n",
        "        for (_, ownP), newP in zip(self.parameters, values):\n",
        "            ownP.data = newP.data\n",
        "    \n",
        "    def to_device(self, device):\n",
        "        for layer in self.layers:\n",
        "            layer.to_device(device)\n",
        "    \n",
        "    def train_mode(self):\n",
        "        for layer in self.layers:\n",
        "            layer.train_mode()\n",
        "    \n",
        "    def eval_mode(self):\n",
        "        for layer in self.layers:\n",
        "            layer.eval_mode()    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def loss(self, outputs, targets):\n",
        "        return self.loss_fn.forward(outputs, targets)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        strs = [\"Feedorward network with %d layers:\" % (len(self.layers),)]\n",
        "        for layer in self.layers:\n",
        "            strs.append(repr(layer))\n",
        "        return '\\n'.join(strs)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRKN6-fssdAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "IrisX = iris.data.astype(np.float32)\n",
        "IrisX = (IrisX - IrisX.mean(axis=0, keepdims=True)) / IrisX.std(axis=0, keepdims=True)\n",
        "IrisY = iris.target\n",
        "\n",
        "IrisX = torch.from_numpy(IrisX)\n",
        "IrisY = torch.from_numpy(IrisY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqFkoqERb5d6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def GD(model, x, y, alpha=1e-4, max_iters=1000000, tolerance=1e-6, device='cpu'):\n",
        "    \"\"\"Simple batch gradient descent\"\"\"\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    model.to_device(device)\n",
        "    try:\n",
        "        old_loss = np.inf\n",
        "        model.train_mode()\n",
        "        tstart = time.time()\n",
        "        for i in xrange(max_iters):\n",
        "            outputs = model.forward(x)\n",
        "            loss = model.loss(outputs, y)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # this disables autograd mechanism and allows us to \n",
        "            with torch.torch.no_grad():\n",
        "                for _, p in model.parameters:\n",
        "                    p -= p.grad * alpha\n",
        "                    # Zero gradients for the next iteration\n",
        "                    p.grad.zero_()\n",
        "\n",
        "            loss = loss.item()\n",
        "            if old_loss < loss:\n",
        "                print \"Iter: %d, loss increased!\" % (i,)\n",
        "            if (old_loss - loss) < tolerance:\n",
        "                print \"Tolerance level reached. Exiting.\"\n",
        "                break\n",
        "            if i % 1000 == 0:\n",
        "                _, predictions = outputs.data.max(dim=1)\n",
        "                err_rate = 100.0 * (predictions != y.data).sum() / outputs.size(0)\n",
        "                print \"Iteration {0: >6} | loss {1: >5.2f} | err rate  {2: >5.2f}% | steps/s {3: >5.2f}\" \\\n",
        "                      .format(i, loss, err_rate, (1000 if i else 1) / (time.time() - tstart))\n",
        "                tstart = time.time()\n",
        "            old_loss = loss\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ju1uDQ4zb5d_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = FeedforwardNet(\n",
        "    [AffineLayer(4, 10),\n",
        "     TanhLayer(),\n",
        "     AffineLayer(10, 3),\n",
        "     SoftMaxLayer(),\n",
        "    ])\n",
        "\n",
        "\n",
        "# Initialize parameters\n",
        "with torch.torch.no_grad():\n",
        "    for n, p in model.parameters:\n",
        "        if n == 'W':\n",
        "            # p.data.normal_(0, 0.05)\n",
        "            p.uniform_(-0.1, 0.1)\n",
        "        elif n == 'b':\n",
        "            p.zero_()\n",
        "        else:\n",
        "            raise ValueError('Unknown parameter name \"%s\"' % p.name)\n",
        "\n",
        "# Train\n",
        "# Iris is so small, that trainingon CPU is faster!\n",
        "GD(model, IrisX, IrisY, alpha=1e-1, tolerance=1e-7, device='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lEkLk_Qb5eD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Starter code for MNIST and SGD scaffolding"
      ]
    },
    {
      "metadata": {
        "id": "YVAA8Twpb5eF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "data_path = './data'\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    ])\n",
        "\n",
        "_test = torchvision.datasets.MNIST(\n",
        "    data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "# Load training data, split into train and valid sets\n",
        "_train = torchvision.datasets.MNIST(\n",
        "    data_path, train=True, download=True, transform=transform)\n",
        "_train.train_data = _train.train_data[:50000]\n",
        "_train.train_labels = _train.train_labels[:50000]\n",
        "\n",
        "_valid = torchvision.datasets.MNIST(\n",
        "    data_path, train=True, download=True, transform=transform)\n",
        "_valid.train_data = _valid.train_data[50000:]\n",
        "_valid.train_labels = _valid.train_labels[50000:]\n",
        "\n",
        "mnist_loaders = {\n",
        "    'train': torch.utils.data.DataLoader(\n",
        "        _train, batch_size=batch_size, shuffle=True,\n",
        "        pin_memory=True, num_workers=10),\n",
        "    'valid': torch.utils.data.DataLoader(\n",
        "        _valid, batch_size=batch_size, shuffle=False),\n",
        "    'test': torch.utils.data.DataLoader(\n",
        "        _test, batch_size=batch_size, shuffle=False)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ABK7MJLUb5eM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Stochastic Gradient Descent [3p]\n",
        "Implement the following additions to the SGD code provided above:\n",
        "  1. **[1p]** momentum\n",
        "  2. **[1p]** learning rate schedule\n",
        "  3. **[1p]** weight decay, in which we additionally minimize for each weight matrix (but typically not the bias) the sum of its elements squared. One way to implement it is to use function `model.parameters` and select all parameters whose names are \"`W`\" and not \"`b`\"."
      ]
    },
    {
      "metadata": {
        "id": "-LXXZgE8b5eM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Tuning the Network for MNIST [4p]\n",
        "\n",
        "Tune the following network to reach **validation error rate below 1.9%**.\n",
        "This should result in a **test error rate below 2%**. To\n",
        "tune the network you will need to:\n",
        "1. Choose the number of layers (more than 1, less than 5);\n",
        "2. Choose the number of neurons in each layer (more than 100,\n",
        "    less than 5000);\n",
        "3. Pick proper weight initialization;\n",
        "4. Pick proper learning rate schedule (need to decay over time,\n",
        "    good range to check on MNIST is about 1e-2 ... 1e-1 at the beginning and\n",
        "    half of that after 10000 batches);\n",
        "5. Pick a momentum constant (probably a constant one will be OK).\n"
      ]
    },
    {
      "metadata": {
        "id": "QaMKIywWb5eI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_error_rate(model, data_loader, device='cpu'):\n",
        "    model.eval_mode()\n",
        "    num_errs = 0.0\n",
        "    num_examples = 0\n",
        "    # we don't need gradient during eval!\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x = x.to(device).view(x.size(0), -1)\n",
        "            y = y.to(device)\n",
        "            outputs = model.forward(x)\n",
        "            _, predictions = outputs.data.max(dim=1)\n",
        "            num_errs += (predictions != y.data).sum().item()\n",
        "            num_examples += x.size(0)\n",
        "    return 100.0 * num_errs / num_examples\n",
        "\n",
        "def SGD(model, data_loaders, alpha=1e-4, num_epochs=1, patience_expansion=1.5,\n",
        "        log_every=100, device='cpu'):\n",
        "    model.to_device(device)\n",
        "    #\n",
        "    # TODO: Initialize momentum variables\n",
        "    # Hint: You need one velocity matrix for each parameter\n",
        "    #\n",
        "    velocities = [None] * len(model.parameters)\n",
        "    #\n",
        "    iter_ = 0\n",
        "    epoch = 0\n",
        "    best_params = None\n",
        "    best_val_err = np.inf\n",
        "    history = {'train_losses': [], 'train_errs': [], 'val_errs': []}\n",
        "    print('Training the model!')\n",
        "    print('Interrupt at any time to evaluate the best validation model so far.')\n",
        "    try:\n",
        "        tstart = time.time()\n",
        "        siter = iter_\n",
        "        while epoch < num_epochs:\n",
        "            model.train_mode()\n",
        "            epoch += 1\n",
        "            for x, y in data_loaders['train']:\n",
        "                x = x.to(device).view(x.size(0), -1)\n",
        "                y = y.to(device)\n",
        "                iter_ += 1\n",
        "                out = model.forward(x)\n",
        "                loss = model.loss(out, y)\n",
        "                loss.backward()\n",
        "                _, predictions = out.max(dim=1)\n",
        "                err_rate = 100.0 * (predictions != y).sum().item() / out.size(0)\n",
        "\n",
        "                history['train_losses'].append(loss.item())\n",
        "                history['train_errs'].append(err_rate)\n",
        "                \n",
        "                # disable gradient computations - we do not want torch to\n",
        "                # backpropagate through the gradient application!\n",
        "                with torch.no_grad():\n",
        "                    for (name, p), v in zip(model.parameters, velocities):\n",
        "                        if name == 'W':\n",
        "                            pass\n",
        "                        #\n",
        "                        # TODO: Implement weight decay addition to gradients\n",
        "                        # p.grad += TODO\n",
        "                        # \n",
        "\n",
        "                        #\n",
        "                        # TODO: Update learning rate\n",
        "                        # Hint: Use the iteration counter i\n",
        "                        # alpha = TODO\n",
        "                        #\n",
        "\n",
        "                        #\n",
        "                        # TODO: Set the momentum constant \n",
        "                        # epsilon = TODO\n",
        "                        #\n",
        "\n",
        "                        #\n",
        "                        # TODO: Implement velocity update in momentum\n",
        "                        # lease make sure to modify the contents of v, not the v pointer!!!\n",
        "                        # v[...] = TODO\n",
        "                        #\n",
        "\n",
        "                        #\n",
        "                        # TODO: Set a more sensible learning rule here,\n",
        "                        #       using your learning rate schedule and momentum\n",
        "                        # \n",
        "                        p -= alpha * p.grad\n",
        "                        # Zero gradients for the next iteration\n",
        "                        p.grad.zero_()\n",
        "\n",
        "                if iter_ % log_every == 0:\n",
        "                    num_iter = iter_ - siter + 1\n",
        "                    print \"Minibatch {0: >6}  | loss {1: >5.2f} | err rate {2: >5.2f}%, steps/s {3: >5.2f}\" \\\n",
        "                          .format(iter_, loss.item(), err_rate, num_iter / (time.time() - tstart))\n",
        "                    tstart = time.time()\n",
        "                           \n",
        "            \n",
        "            val_err_rate = compute_error_rate(model, data_loaders['valid'], device)\n",
        "            history['val_errs'].append((iter_, val_err_rate))\n",
        "            \n",
        "            if val_err_rate < best_val_err:\n",
        "                # Adjust num of epochs\n",
        "                num_epochs = int(np.maximum(num_epochs, epoch * patience_expansion + 1))\n",
        "                best_epoch = epoch\n",
        "                best_val_err = val_err_rate\n",
        "                best_params = [p.detach().cpu() for (_, p) in model.parameters]\n",
        "            m = \"After epoch {0: >2} | valid err rate: {1: >5.2f}% | doing {2: >3} epochs\" \\\n",
        "                .format(epoch, val_err_rate, num_epochs)\n",
        "            print '{0}\\n{1}\\n{0}'.format('-' * len(m), m)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    if best_params is not None:\n",
        "        print \"\\nLoading best params on validation set (epoch %d)\\n\" %(best_epoch)\n",
        "        model.parameters = best_params\n",
        "    plot_history(history)\n",
        "\n",
        "def plot_history(history):\n",
        "    figsize(16, 4)\n",
        "    subplot(1,2,1)\n",
        "    train_loss = np.array(history['train_losses'])\n",
        "    semilogy(np.arange(train_loss.shape[0]), train_loss, label='batch train loss')\n",
        "    legend()\n",
        "        \n",
        "    subplot(1,2,2)\n",
        "    train_errs = np.array(history['train_errs'])\n",
        "    plot(np.arange(train_errs.shape[0]), train_errs, label='batch train error rate')\n",
        "    val_errs = np.array(history['val_errs'])\n",
        "    plot(val_errs[:,0], val_errs[:,1], label='validation error rate', color='r')\n",
        "    ylim(0,20)\n",
        "    legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "VvGfryojb5eO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# TODO: Pick a network architecture here.\n",
        "#       The one below is just softmax regression.\n",
        "#\n",
        "\n",
        "model = FeedforwardNet(\n",
        "    [\n",
        "     AffineLayer(784, 10),\n",
        "     SoftMaxLayer()\n",
        "    ])\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Initialize parameters\n",
        "    for name, p in model.parameters:\n",
        "        if name == 'W':\n",
        "            p.normal_(0, 0.05)\n",
        "            # p.data.uniform_(-0.1, 0.1)\n",
        "        elif name == 'b':\n",
        "            p.zero_()\n",
        "        else:\n",
        "            raise ValueError('Unknown parameter name \"%s\"' % p.name)\n",
        "\n",
        "# On GPU enabled devices set device='cuda' else set device='cpu'\n",
        "SGD(model, mnist_loaders, alpha=1e-1, device='cuda')\n",
        "\n",
        "print \"Test error rate: %.2f%%\" % compute_error_rate(model, mnist_loaders['test'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LP9RLV0bb5eT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Dropout [2p]\n",
        "\n",
        "Implement a **dropout** layer and try to train a\n",
        "network getting below 1.5% test error rates with dropout. The best\n",
        "results with dropout are below 1%!\n",
        "\n",
        "Remember to turn off dropout during testing, using `model.train_mode()` and `model.eval_mode()`!\n",
        "\n",
        "Hint: Use [torch.nn.functional.dropout](http://pytorch.org/docs/master/nn.html#torch.nn.functional.dropout).\n",
        "\n",
        "Details: http://arxiv.org/pdf/1207.0580.pdf."
      ]
    },
    {
      "metadata": {
        "id": "9deDKuMfb5eU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 4: Data Augmentation [1p]\n",
        "\n",
        "Apply data augmentation methods (e.g. rotations, noise, crops) when training networks on MNIST, to significantly reduce test error rate for your network. You can use functions from the [torchvision.transforms](http://pytorch.org/docs/master/torchvision/transforms.html) module."
      ]
    },
    {
      "metadata": {
        "id": "G9b4T19Hb5eW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 5: Batch Normalization [1p]\n",
        "\n",
        "*Covariate shift* is a phenomenon associated with training deep models. Simply put, weight changes in early layers cause major changes in distribution of inputs to later layers, making it difficult to train later layers.\n",
        "\n",
        "[Batch Normalization](https://arxiv.org/abs/1502.03167) addresses this problem by normalizing distributions of inputs to layers within mini-batches. It typically allows to train networks faster and/or with higher learning rates, lessens the importance\n",
        "of initialization and might eliminate the need for Dropout.\n",
        "\n",
        "Implement Batch Normalization and compare with regular training of MNIST models.\n",
        "\n",
        "Remember to use the batch statistics during model training and to use an average of training batch statistics during model evaluation. For details please consult the paper."
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "NYYZv37cb5eY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 6: Norm Constraints [1p bonus]\n",
        "\n",
        "Implement norm constraints, i.e. instead of weight decay, that tries to set \n",
        "all weights to small values, apply a limit on the total\n",
        "norm of connections incoming to a neuron. In our case, this\n",
        "corresponds to clipping the norm of *rows* of weight\n",
        "matrices. An easy way of implementing it is to make a gradient\n",
        "step, then look at the norm of rows and scale down those that are\n",
        "over the threshold (this technique is called \"projected gradient descent\").\n",
        "\n",
        "Please consult the Dropout paper (http://arxiv.org/pdf/1207.0580.pdf) for details."
      ]
    },
    {
      "metadata": {
        "id": "F7RPI8HNb5eY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 6: Polyak Averaging [1p bonus]\n",
        "\n",
        "Implement Polyak averaging. For each parameter $\\theta$\n",
        "keep a separate, exponentially decayed average of the past values\n",
        "$$\n",
        "\\bar{\\theta}_n = \\alpha_p\\bar{\\theta}_{n-1} + (1-\\alpha_p)\\theta_n.\n",
        "$$\n",
        "Use that average when evaluating the model on the test set.\n",
        "Validate the approach by training a model on the MNIST dataset."
      ]
    },
    {
      "metadata": {
        "id": "4kpFI3lkb5ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 7: Convolutional Network [2p bonus]\n",
        "\n",
        "Use convolutional and max-pooling layers (`torch.nn.functional.conv2d`, `torch.nn.functional.max_pool2d`) and (without dropout) get a test error rate below 1.5%."
      ]
    },
    {
      "metadata": {
        "id": "kA1Glom1b5ee",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 8: Hyperparameter tuner [1p bonus]\n",
        "\n",
        "Implement a hyper-parameter tuner able to optimize the learing rate schedule, number of neurons and similar hyperparameters. For start, use random search (please see http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf and especially Fig 1. for intuitions on why random search is better than grid search). It may be a good idea to use a fixed maximum number of epochs (or training time) for each optimization trial to prevent selecting hyperparameters that yield slowly converging solutions. A good result will be a set of hyperparameters that reach on MNIST solutions with test errors less than $1.3\\%$ in no more than 50 epochs."
      ]
    },
    {
      "metadata": {
        "id": "pmY0y9O-b5ej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
